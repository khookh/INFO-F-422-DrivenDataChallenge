{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting classifier with decision trees\n",
    "\n",
    "Gradient boosting is an ensemble learning technique that consists of sequentially adding predictors. Each predictor will fit on the residual error of the previous predictor.\n",
    "\n",
    "The predictors used in this case are weak learners decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "\n",
      "Loading required package: ggplot2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(xgboost)\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_drop<-c(\"X.1\",\"X\")\n",
    "training_set <- read.csv(\"../Data/PreProcess/processed_training_data_split.csv\") # load training set\n",
    "validation_set <- read.csv(\"../Data/PreProcess/processed_verification_data_split.csv\") # load validation set\n",
    "\n",
    "# Dropped \"X.1\",\"X\" because they just represent the row numbers\n",
    "training_set<-training_set[,!(names(validation_set) %in% column_to_drop)] # drop the desired columns on validation set\n",
    "validation_set<-validation_set[,!(names(validation_set) %in% column_to_drop)] # drop the desired columns on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using  trees...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test GBM - fait planter le kernel \n",
    "library(gbm)\n",
    "gbm.fit <- gbm(\n",
    "  formula = id ~ .,\n",
    "  distribution = \"gaussian\",\n",
    "  data = training_set,\n",
    "  n.trees = 10,\n",
    "  interaction.depth = 1,\n",
    "  shrinkage = 0.9,\n",
    "  cv.folds = 5,\n",
    "  verbose = FALSE\n",
    "  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>nrounds</th><th scope=col>max_depth</th><th scope=col>eta</th><th scope=col>gamma</th><th scope=col>colsample_bytree</th><th scope=col>min_child_weight</th><th scope=col>subsample</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>102</th><td>150</td><td>3</td><td>0.4</td><td>0</td><td>0.8</td><td>1</td><td>0.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 7\n",
       "\\begin{tabular}{r|lllllll}\n",
       "  & nrounds & max\\_depth & eta & gamma & colsample\\_bytree & min\\_child\\_weight & subsample\\\\\n",
       "  & <dbl> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t102 & 150 & 3 & 0.4 & 0 & 0.8 & 1 & 0.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 7\n",
       "\n",
       "| <!--/--> | nrounds &lt;dbl&gt; | max_depth &lt;int&gt; | eta &lt;dbl&gt; | gamma &lt;dbl&gt; | colsample_bytree &lt;dbl&gt; | min_child_weight &lt;dbl&gt; | subsample &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "| 102 | 150 | 3 | 0.4 | 0 | 0.8 | 1 | 0.5 |\n",
       "\n"
      ],
      "text/plain": [
       "    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample\n",
       "102 150     3         0.4 0     0.8              1                0.5      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## test xgboost\n",
    "modelGB <- train(\n",
    "  id ~., data = training_set, method = \"xgbTree\",\n",
    "  trControl = trainControl(\"cv\", number = 10)\n",
    "  )\n",
    "# tuning parameter\n",
    "modelGB$bestTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eXtreme Gradient Boosting \n",
       "\n",
       "47289 samples\n",
       "   83 predictor\n",
       "    3 classes: 'functional', 'functional needs repair', 'non functional' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 42560, 42561, 42559, 42561, 42560, 42561, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy   Kappa    \n",
       "  0.3  1          0.6               0.50        50      0.7126180  0.4180032\n",
       "  0.3  1          0.6               0.50       100      0.7211823  0.4408679\n",
       "  0.3  1          0.6               0.50       150      0.7249252  0.4513928\n",
       "  0.3  1          0.6               0.75        50      0.7119624  0.4165799\n",
       "  0.3  1          0.6               0.75       100      0.7204421  0.4386828\n",
       "  0.3  1          0.6               0.75       150      0.7241851  0.4491404\n",
       "  0.3  1          0.6               1.00        50      0.7115183  0.4147708\n",
       "  0.3  1          0.6               1.00       100      0.7196174  0.4363322\n",
       "  0.3  1          0.6               1.00       150      0.7240582  0.4480957\n",
       "  0.3  1          0.8               0.50        50      0.7129564  0.4187012\n",
       "  0.3  1          0.8               0.50       100      0.7214361  0.4415259\n",
       "  0.3  1          0.8               0.50       150      0.7249887  0.4518713\n",
       "  0.3  1          0.8               0.75        50      0.7128717  0.4185169\n",
       "  0.3  1          0.8               0.75       100      0.7203787  0.4388115\n",
       "  0.3  1          0.8               0.75       150      0.7247350  0.4502924\n",
       "  0.3  1          0.8               1.00        50      0.7120259  0.4161139\n",
       "  0.3  1          0.8               1.00       100      0.7199769  0.4369664\n",
       "  0.3  1          0.8               1.00       150      0.7246080  0.4490474\n",
       "  0.3  2          0.6               0.50        50      0.7297257  0.4607053\n",
       "  0.3  2          0.6               0.50       100      0.7392415  0.4844142\n",
       "  0.3  2          0.6               0.50       150      0.7438092  0.4958683\n",
       "  0.3  2          0.6               0.75        50      0.7289432  0.4585242\n",
       "  0.3  2          0.6               0.75       100      0.7371693  0.4802213\n",
       "  0.3  2          0.6               0.75       150      0.7431958  0.4945074\n",
       "  0.3  2          0.6               1.00        50      0.7278225  0.4556578\n",
       "  0.3  2          0.6               1.00       100      0.7364924  0.4775243\n",
       "  0.3  2          0.6               1.00       150      0.7407850  0.4890827\n",
       "  0.3  2          0.8               0.50        50      0.7309311  0.4630774\n",
       "  0.3  2          0.8               0.50       100      0.7389877  0.4845865\n",
       "  0.3  2          0.8               0.50       150      0.7445280  0.4979361\n",
       "  0.3  2          0.8               0.75        50      0.7286048  0.4577333\n",
       "  0.3  2          0.8               0.75       100      0.7379515  0.4817532\n",
       "  0.3  2          0.8               0.75       150      0.7445704  0.4969558\n",
       "  0.3  2          0.8               1.00        50      0.7283087  0.4571692\n",
       "  0.3  2          0.8               1.00       100      0.7367885  0.4785134\n",
       "  0.3  2          0.8               1.00       150      0.7419482  0.4915422\n",
       "  0.3  3          0.6               0.50        50      0.7425191  0.4894627\n",
       "  0.3  3          0.6               0.50       100      0.7534097  0.5165326\n",
       "  0.3  3          0.6               0.50       150      0.7595420  0.5307479\n",
       "  0.3  3          0.6               0.75        50      0.7423500  0.4890724\n",
       "  0.3  3          0.6               0.75       100      0.7526904  0.5142331\n",
       "  0.3  3          0.6               0.75       150      0.7585481  0.5281194\n",
       "  0.3  3          0.6               1.00        50      0.7404890  0.4844826\n",
       "  0.3  3          0.6               1.00       100      0.7515697  0.5112103\n",
       "  0.3  3          0.6               1.00       150      0.7590556  0.5283525\n",
       "  0.3  3          0.8               0.50        50      0.7436400  0.4929360\n",
       "  0.3  3          0.8               0.50       100      0.7538960  0.5180381\n",
       "  0.3  3          0.8               0.50       150      0.7592035  0.5308717\n",
       "  0.3  3          0.8               0.75        50      0.7427729  0.4903470\n",
       "  0.3  3          0.8               0.75       100      0.7535364  0.5160012\n",
       "  0.3  3          0.8               0.75       150      0.7605359  0.5323405\n",
       "  0.3  3          0.8               1.00        50      0.7421807  0.4882734\n",
       "  0.3  3          0.8               1.00       100      0.7518023  0.5116640\n",
       "  0.3  3          0.8               1.00       150      0.7582097  0.5265674\n",
       "  0.4  1          0.6               0.50        50      0.7162550  0.4278311\n",
       "  0.4  1          0.6               0.50       100      0.7235719  0.4482257\n",
       "  0.4  1          0.6               0.50       150      0.7267014  0.4572425\n",
       "  0.4  1          0.6               0.75        50      0.7154939  0.4257358\n",
       "  0.4  1          0.6               0.75       100      0.7231489  0.4464843\n",
       "  0.4  1          0.6               0.75       150      0.7268919  0.4568129\n",
       "  0.4  1          0.6               1.00        50      0.7154727  0.4254756\n",
       "  0.4  1          0.6               1.00       100      0.7227048  0.4450122\n",
       "  0.4  1          0.6               1.00       150      0.7259192  0.4538399\n",
       "  0.4  1          0.8               0.50        50      0.7165089  0.4283753\n",
       "  0.4  1          0.8               0.50       100      0.7239525  0.4490226\n",
       "  0.4  1          0.8               0.50       150      0.7271879  0.4584663\n",
       "  0.4  1          0.8               0.75        50      0.7157053  0.4260325\n",
       "  0.4  1          0.8               0.75       100      0.7236988  0.4478378\n",
       "  0.4  1          0.8               0.75       150      0.7269766  0.4569479\n",
       "  0.4  1          0.8               1.00        50      0.7155995  0.4258088\n",
       "  0.4  1          0.8               1.00       100      0.7222607  0.4439944\n",
       "  0.4  1          0.8               1.00       150      0.7265112  0.4549002\n",
       "  0.4  2          0.6               0.50        50      0.7334262  0.4701375\n",
       "  0.4  2          0.6               0.50       100      0.7426039  0.4933586\n",
       "  0.4  2          0.6               0.50       150      0.7486940  0.5070968\n",
       "  0.4  2          0.6               0.75        50      0.7331935  0.4693117\n",
       "  0.4  2          0.6               0.75       100      0.7405525  0.4883957\n",
       "  0.4  2          0.6               0.75       150      0.7471925  0.5040328\n",
       "  0.4  2          0.6               1.00        50      0.7321997  0.4668548\n",
       "  0.4  2          0.6               1.00       100      0.7407851  0.4883918\n",
       "  0.4  2          0.6               1.00       150      0.7452681  0.4995338\n",
       "  0.4  2          0.8               0.50        50      0.7354351  0.4743803\n",
       "  0.4  2          0.8               0.50       100      0.7440841  0.4967357\n",
       "  0.4  2          0.8               0.50       150      0.7490324  0.5084348\n",
       "  0.4  2          0.8               0.75        50      0.7321787  0.4673031\n",
       "  0.4  2          0.8               0.75       100      0.7414617  0.4904311\n",
       "  0.4  2          0.8               0.75       150      0.7475731  0.5048042\n",
       "  0.4  2          0.8               1.00        50      0.7330033  0.4686240\n",
       "  0.4  2          0.8               1.00       100      0.7404890  0.4879478\n",
       "  0.4  2          0.8               1.00       150      0.7453315  0.4995078\n",
       "  0.4  3          0.6               0.50        50      0.7478690  0.5028247\n",
       "  0.4  3          0.6               0.50       100      0.7574062  0.5261935\n",
       "  0.4  3          0.6               0.50       150      0.7637290  0.5404995\n",
       "  0.4  3          0.6               0.75        50      0.7463888  0.4993932\n",
       "  0.4  3          0.6               0.75       100      0.7575543  0.5258124\n",
       "  0.4  3          0.6               0.75       150      0.7636021  0.5399367\n",
       "  0.4  3          0.6               1.00        50      0.7454795  0.4968284\n",
       "  0.4  3          0.6               1.00       100      0.7560317  0.5220156\n",
       "  0.4  3          0.6               1.00       150      0.7619103  0.5356192\n",
       "  0.4  3          0.8               0.50        50      0.7488631  0.5056482\n",
       "  0.4  3          0.8               0.50       100      0.7593303  0.5301756\n",
       "  0.4  3          0.8               0.50       150      0.7651033  0.5440059\n",
       "  0.4  3          0.8               0.75        50      0.7466003  0.5000454\n",
       "  0.4  3          0.8               0.75       100      0.7579772  0.5273841\n",
       "  0.4  3          0.8               0.75       150      0.7640252  0.5418553\n",
       "  0.4  3          0.8               1.00        50      0.7470658  0.5008248\n",
       "  0.4  3          0.8               1.00       100      0.7566661  0.5234004\n",
       "  0.4  3          0.8               1.00       150      0.7640039  0.5401277\n",
       "\n",
       "Tuning parameter 'gamma' was held constant at a value of 0\n",
       "Tuning\n",
       " parameter 'min_child_weight' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were nrounds = 150, max_depth = 3, eta\n",
       " = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample\n",
       " = 0.5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelGB\n",
    "save(modelGB,file = \"4-Models/modelGB.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "                         Reference\n",
       "Prediction                functional functional needs repair non functional\n",
       "  functional                    5438                     642           1963\n",
       "  functional needs repair         55                      80             34\n",
       "  non functional                1030                     195           2673\n",
       "\n",
       "Overall Statistics\n",
       "                                         \n",
       "               Accuracy : 0.6764         \n",
       "                 95% CI : (0.668, 0.6847)\n",
       "    No Information Rate : 0.5386         \n",
       "    P-Value [Acc > NIR] : < 2.2e-16      \n",
       "                                         \n",
       "                  Kappa : 0.3741         \n",
       "                                         \n",
       " Mcnemar's Test P-Value : < 2.2e-16      \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: functional Class: functional needs repair\n",
       "Sensitivity                     0.8337                       0.087241\n",
       "Specificity                     0.5337                       0.992049\n",
       "Pos Pred Value                  0.6761                       0.473373\n",
       "Neg Pred Value                  0.7332                       0.929905\n",
       "Prevalence                      0.5386                       0.075723\n",
       "Detection Rate                  0.4491                       0.006606\n",
       "Detection Prevalence            0.6642                       0.013955\n",
       "Balanced Accuracy               0.6837                       0.539645\n",
       "                     Class: non functional\n",
       "Sensitivity                         0.5724\n",
       "Specificity                         0.8353\n",
       "Pos Pred Value                      0.6857\n",
       "Neg Pred Value                      0.7568\n",
       "Prevalence                          0.3856\n",
       "Detection Rate                      0.2207\n",
       "Detection Prevalence                0.3219\n",
       "Balanced Accuracy                   0.7039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"4-Models/modelGB.RData\")\n",
    "pred <- predict(modelGB,validation_set)\n",
    "conf <- confusionMatrix(data=pred, reference = factor(validation_set$id[-1])) # GBM confusion matrix\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
