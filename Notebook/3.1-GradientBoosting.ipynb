{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting classifier with decision trees\n",
    "\n",
    "Gradient boosting is an ensemble learning technique that consists of sequentially adding predictors. Each predictor will fit on the residual error of the previous predictor.\n",
    "\n",
    "The predictors used in this case are weak learners decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_drop<-c(\"X.1\",\"X\")\n",
    "training_set <- read.csv(\"../Data/PreProcess/processed_training_data_split.csv\") # load training set\n",
    "validation_set <- read.csv(\"../Data/PreProcess/processed_verification_data_split.csv\") # load validation set\n",
    "\n",
    "# Dropped \"X.1\",\"X\" because they just represent the row numbers\n",
    "training_set<-training_set[,!(names(validation_set) %in% column_to_drop)] # drop the desired columns on validation set\n",
    "validation_set<-validation_set[,!(names(validation_set) %in% column_to_drop)] # drop the desired columns on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using  trees...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test GBM - fait planter le kernel \n",
    "library(gbm)\n",
    "gbm.fit <- gbm(\n",
    "  formula = id ~ .,\n",
    "  distribution = \"gaussian\",\n",
    "  data = training_set,\n",
    "  n.trees = 10,\n",
    "  interaction.depth = 1,\n",
    "  shrinkage = 0.9,\n",
    "  cv.folds = 5,\n",
    "  verbose = FALSE\n",
    "  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>nrounds</th><th scope=col>max_depth</th><th scope=col>eta</th><th scope=col>gamma</th><th scope=col>colsample_bytree</th><th scope=col>min_child_weight</th><th scope=col>subsample</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>102</th><td>150</td><td>3</td><td>0.4</td><td>0</td><td>0.8</td><td>1</td><td>0.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 7\n",
       "\\begin{tabular}{r|lllllll}\n",
       "  & nrounds & max\\_depth & eta & gamma & colsample\\_bytree & min\\_child\\_weight & subsample\\\\\n",
       "  & <dbl> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t102 & 150 & 3 & 0.4 & 0 & 0.8 & 1 & 0.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 7\n",
       "\n",
       "| <!--/--> | nrounds &lt;dbl&gt; | max_depth &lt;int&gt; | eta &lt;dbl&gt; | gamma &lt;dbl&gt; | colsample_bytree &lt;dbl&gt; | min_child_weight &lt;dbl&gt; | subsample &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "| 102 | 150 | 3 | 0.4 | 0 | 0.8 | 1 | 0.5 |\n",
       "\n"
      ],
      "text/plain": [
       "    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample\n",
       "102 150     3         0.4 0     0.8              1                0.5      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## test xgboost\n",
    "modelGB <- train(\n",
    "  id ~., data = training_set, method = \"xgbTree\",\n",
    "  trControl = trainControl(\"cv\", number = 3)\n",
    "  )\n",
    "# tuning parameter\n",
    "modelGB$bestTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eXtreme Gradient Boosting \n",
       "\n",
       "47289 samples\n",
       "   83 predictor\n",
       "    3 classes: 'functional', 'functional needs repair', 'non functional' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (3 fold) \n",
       "Summary of sample sizes: 31527, 31526, 31525 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy   Kappa    \n",
       "  0.3  1          0.6               0.50        50      0.7125337  0.4177527\n",
       "  0.3  1          0.6               0.50       100      0.7206963  0.4398183\n",
       "  0.3  1          0.6               0.50       150      0.7240163  0.4499367\n",
       "  0.3  1          0.6               0.75        50      0.7122800  0.4166842\n",
       "  0.3  1          0.6               0.75       100      0.7199561  0.4376988\n",
       "  0.3  1          0.6               0.75       150      0.7241855  0.4490711\n",
       "  0.3  1          0.6               1.00        50      0.7104614  0.4130898\n",
       "  0.3  1          0.6               1.00       100      0.7194064  0.4357767\n",
       "  0.3  1          0.6               1.00       150      0.7227475  0.4450849\n",
       "  0.3  1          0.8               0.50        50      0.7129778  0.4193912\n",
       "  0.3  1          0.8               0.50       100      0.7215210  0.4416684\n",
       "  0.3  1          0.8               0.50       150      0.7249679  0.4518910\n",
       "  0.3  1          0.8               0.75        50      0.7116879  0.4161416\n",
       "  0.3  1          0.8               0.75       100      0.7202522  0.4383867\n",
       "  0.3  1          0.8               0.75       150      0.7236779  0.4483352\n",
       "  0.3  1          0.8               1.00        50      0.7106517  0.4135987\n",
       "  0.3  1          0.8               1.00       100      0.7194275  0.4362925\n",
       "  0.3  1          0.8               1.00       150      0.7225995  0.4449019\n",
       "  0.3  2          0.6               0.50        50      0.7285417  0.4579347\n",
       "  0.3  2          0.6               0.50       100      0.7375924  0.4822636\n",
       "  0.3  2          0.6               0.50       150      0.7433654  0.4954535\n",
       "  0.3  2          0.6               0.75        50      0.7283936  0.4574021\n",
       "  0.3  2          0.6               0.75       100      0.7358795  0.4779009\n",
       "  0.3  2          0.6               0.75       150      0.7436192  0.4960694\n",
       "  0.3  2          0.6               1.00        50      0.7281188  0.4561040\n",
       "  0.3  2          0.6               1.00       100      0.7369580  0.4794373\n",
       "  0.3  2          0.6               1.00       150      0.7421178  0.4918299\n",
       "  0.3  2          0.8               0.50        50      0.7302334  0.4619142\n",
       "  0.3  2          0.8               0.50       100      0.7396859  0.4873011\n",
       "  0.3  2          0.8               0.50       150      0.7444650  0.4985200\n",
       "  0.3  2          0.8               0.75        50      0.7285205  0.4577711\n",
       "  0.3  2          0.8               0.75       100      0.7387766  0.4836045\n",
       "  0.3  2          0.8               0.75       150      0.7439575  0.4966188\n",
       "  0.3  2          0.8               1.00        50      0.7297047  0.4592488\n",
       "  0.3  2          0.8               1.00       100      0.7366619  0.4781040\n",
       "  0.3  2          0.8               1.00       150      0.7416525  0.4905241\n",
       "  0.3  3          0.6               0.50        50      0.7424350  0.4898218\n",
       "  0.3  3          0.6               0.50       100      0.7533043  0.5163243\n",
       "  0.3  3          0.6               0.50       150      0.7593733  0.5305192\n",
       "  0.3  3          0.6               0.75        50      0.7415680  0.4876873\n",
       "  0.3  3          0.6               0.75       100      0.7536216  0.5162548\n",
       "  0.3  3          0.6               0.75       150      0.7599444  0.5314930\n",
       "  0.3  3          0.6               1.00        50      0.7394110  0.4826814\n",
       "  0.3  3          0.6               1.00       100      0.7511474  0.5106874\n",
       "  0.3  3          0.6               1.00       150      0.7583795  0.5273101\n",
       "  0.3  3          0.8               0.50        50      0.7423082  0.4901854\n",
       "  0.3  3          0.8               0.50       100      0.7532197  0.5165941\n",
       "  0.3  3          0.8               0.50       150      0.7600078  0.5323919\n",
       "  0.3  3          0.8               0.75        50      0.7429213  0.4909201\n",
       "  0.3  3          0.8               0.75       100      0.7540022  0.5177960\n",
       "  0.3  3          0.8               0.75       150      0.7596694  0.5312855\n",
       "  0.3  3          0.8               1.00        50      0.7415891  0.4874727\n",
       "  0.3  3          0.8               1.00       100      0.7513800  0.5115748\n",
       "  0.3  3          0.8               1.00       150      0.7578086  0.5265723\n",
       "  0.4  1          0.6               0.50        50      0.7161287  0.4280822\n",
       "  0.4  1          0.6               0.50       100      0.7235933  0.4481740\n",
       "  0.4  1          0.6               0.50       150      0.7266808  0.4571884\n",
       "  0.4  1          0.6               0.75        50      0.7157268  0.4262691\n",
       "  0.4  1          0.6               0.75       100      0.7232551  0.4468781\n",
       "  0.4  1          0.6               0.75       150      0.7261944  0.4553639\n",
       "  0.4  1          0.6               1.00        50      0.7159806  0.4263056\n",
       "  0.4  1          0.6               1.00       100      0.7219651  0.4432245\n",
       "  0.4  1          0.6               1.00       150      0.7256657  0.4532962\n",
       "  0.4  1          0.8               0.50        50      0.7155154  0.4264950\n",
       "  0.4  1          0.8               0.50       100      0.7233608  0.4482793\n",
       "  0.4  1          0.8               0.50       150      0.7268289  0.4580334\n",
       "  0.4  1          0.8               0.75        50      0.7151348  0.4253186\n",
       "  0.4  1          0.8               0.75       100      0.7234665  0.4471486\n",
       "  0.4  1          0.8               0.75       150      0.7266173  0.4565797\n",
       "  0.4  1          0.8               1.00        50      0.7150079  0.4245535\n",
       "  0.4  1          0.8               1.00       100      0.7226629  0.4445310\n",
       "  0.4  1          0.8               1.00       150      0.7256023  0.4529609\n",
       "  0.4  2          0.6               0.50        50      0.7321790  0.4680044\n",
       "  0.4  2          0.6               0.50       100      0.7419063  0.4924505\n",
       "  0.4  2          0.6               0.50       150      0.7484405  0.5071295\n",
       "  0.4  2          0.6               0.75        50      0.7336591  0.4707920\n",
       "  0.4  2          0.6               0.75       100      0.7409971  0.4907775\n",
       "  0.4  2          0.6               0.75       150      0.7470450  0.5046704\n",
       "  0.4  2          0.6               1.00        50      0.7298106  0.4618578\n",
       "  0.4  2          0.6               1.00       100      0.7390726  0.4850460\n",
       "  0.4  2          0.6               1.00       150      0.7441902  0.4975416\n",
       "  0.4  2          0.8               0.50        50      0.7330459  0.4701508\n",
       "  0.4  2          0.8               0.50       100      0.7430693  0.4945480\n",
       "  0.4  2          0.8               0.50       150      0.7480176  0.5058810\n",
       "  0.4  2          0.8               0.75        50      0.7324326  0.4684372\n",
       "  0.4  2          0.8               0.75       100      0.7418428  0.4920051\n",
       "  0.4  2          0.8               0.75       150      0.7468335  0.5039612\n",
       "  0.4  2          0.8               1.00        50      0.7325172  0.4675259\n",
       "  0.4  2          0.8               1.00       100      0.7400243  0.4868586\n",
       "  0.4  2          0.8               1.00       150      0.7438940  0.4965307\n",
       "  0.4  3          0.6               0.50        50      0.7476793  0.5027550\n",
       "  0.4  3          0.6               0.50       100      0.7564974  0.5243146\n",
       "  0.4  3          0.6               0.50       150      0.7627991  0.5396833\n",
       "  0.4  3          0.6               0.75        50      0.7473622  0.5015478\n",
       "  0.4  3          0.6               0.75       100      0.7580834  0.5270832\n",
       "  0.4  3          0.6               0.75       150      0.7632855  0.5393891\n",
       "  0.4  3          0.6               1.00        50      0.7456492  0.4968672\n",
       "  0.4  3          0.6               1.00       100      0.7545730  0.5192074\n",
       "  0.4  3          0.6               1.00       150      0.7610862  0.5342940\n",
       "  0.4  3          0.8               0.50        50      0.7478484  0.5039713\n",
       "  0.4  3          0.8               0.50       100      0.7598597  0.5319567\n",
       "  0.4  3          0.8               0.50       150      0.7642582  0.5430383\n",
       "  0.4  3          0.8               0.75        50      0.7468757  0.5011431\n",
       "  0.4  3          0.8               0.75       100      0.7585909  0.5286646\n",
       "  0.4  3          0.8               0.75       150      0.7630318  0.5398492\n",
       "  0.4  3          0.8               1.00        50      0.7458184  0.4984439\n",
       "  0.4  3          0.8               1.00       100      0.7565186  0.5234458\n",
       "  0.4  3          0.8               1.00       150      0.7611497  0.5345080\n",
       "\n",
       "Tuning parameter 'gamma' was held constant at a value of 0\n",
       "Tuning\n",
       " parameter 'min_child_weight' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were nrounds = 150, max_depth = 3, eta\n",
       " = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample\n",
       " = 0.5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelGB\n",
    "save(modelGB,file = \"4-Models/modelGB.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "                         Reference\n",
       "Prediction                functional functional needs repair non functional\n",
       "  functional                    5446                     630           1960\n",
       "  functional needs repair         58                      83             36\n",
       "  non functional                1019                     204           2674\n",
       "\n",
       "Overall Statistics\n",
       "                                         \n",
       "               Accuracy : 0.6774         \n",
       "                 95% CI : (0.669, 0.6857)\n",
       "    No Information Rate : 0.5386         \n",
       "    P-Value [Acc > NIR] : < 2.2e-16      \n",
       "                                         \n",
       "                  Kappa : 0.3764         \n",
       "                                         \n",
       " Mcnemar's Test P-Value : < 2.2e-16      \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: functional Class: functional needs repair\n",
       "Sensitivity                     0.8349                       0.090513\n",
       "Specificity                     0.5364                       0.991602\n",
       "Pos Pred Value                  0.6777                       0.468927\n",
       "Neg Pred Value                  0.7356                       0.930110\n",
       "Prevalence                      0.5386                       0.075723\n",
       "Detection Rate                  0.4497                       0.006854\n",
       "Detection Prevalence            0.6636                       0.014616\n",
       "Balanced Accuracy               0.6857                       0.541057\n",
       "                     Class: non functional\n",
       "Sensitivity                         0.5726\n",
       "Specificity                         0.8356\n",
       "Pos Pred Value                      0.6862\n",
       "Neg Pred Value                      0.7570\n",
       "Prevalence                          0.3856\n",
       "Detection Rate                      0.2208\n",
       "Detection Prevalence                0.3218\n",
       "Balanced Accuracy                   0.7041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"4-Models/modelGB.RData\")\n",
    "pred <- predict(modelGB,validation_set)\n",
    "conf <- confusionMatrix(data=pred, reference = factor(validation_set$id[-1])) # GBM confusion matrix\n",
    "conf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
